\input{../YKY-preamble.tex}
% \usepackage[no-math]{fontspec}
% \setmainfont[BoldFont=Alibaba_Sans_Regular.otf,ItalicFont=Alibaba_Sans_Light_Italic.otf]{Alibaba_Sans_Light.otf}

\usepackage[backend=biber]{biblatex}
\bibliography{../AGI-book}

\usepackage[active,tightpage]{preview}		% for continuous page(s)
\renewcommand{\PreviewBorder}{0.5cm}
\renewcommand{\thempfootnote}{\arabic{mpfootnote}}

\usepackage[absolute,overlay]{textpos}		% for page number on upper left corner
\usepackage{wrapfig}						% wrap text around figure

\usepackage{color}
% \usepackage{mathtools}
\usepackage[hyperfootnotes=false]{hyperref}

% \usepackage[backend=biber,style=numeric]{biblatex}
% \bibliography{../AGI-book}
% \renewcommand*{\bibfont}{\footnotesize}

\usetikzlibrary{shapes}
% \usepackage[export]{adjustbox}	% ??
\usepackage{verbatim} % for comments
% \usepackage{newtxtext,newtxmath}	% Times New Roman font

% \titleformat{\subsection}[hang]{\bfseries\large\color{blue}}{}{0pt}{} 
% \numberwithin{equation}{subsection}

\newcommand{\underdash}[1]{%
	\tikz[baseline=(toUnderline.base)]{
		\node[inner sep=1pt,outer sep=10pt] (toUnderline) {#1};
		\draw[dashed] ([yshift=-0pt]toUnderline.south west) -- ([yshift=-0pt]toUnderline.south east);
	}%
}%

\newcommand\reduline{\bgroup\markoverwith{\textcolor{red}{\rule[-0.5ex]{2pt}{0.4pt}}}\ULon}

%\DeclareSymbolFont{symbolsC}{U}{txsyc}{m}{n}
%\DeclareMathSymbol{\strictif}{\mathrel}{symbolsC}{74}
%\DeclareSymbolFont{AMSb}{U}{msb}{m}{n}
%\DeclareSymbolFontAlphabet{\mathbb}{AMSb}
%\setmathfont{lmroman17-regular.otf}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% \usepackage[most]{tcolorbox}
%\tcbset{on line, 
%	boxsep=4pt, left=0pt,right=0pt,top=0pt,bottom=0pt,
%	colframe=red,colback=pink,
%	highlight math style={enhanced}
%}
%\newcommand{\atom}{\vcenter{\hbox{\tcbox{....}}}}

\let\oldtextbf\textbf
\renewcommand{\textbf}[1]{\textcolor{blue}{\oldtextbf{#1}}}

\newcommand{\logic}[1]{{\color{violet}{\textit{#1}}}}
\newcommand{\underconst}{\includegraphics[scale=0.5]{../2020/UnderConst.png}}
\newcommand{\KBsymbol}{\vcenter{\hbox{\includegraphics[scale=1]{../KB-symbol.png}}}}
\newcommand{\token}{\vcenter{\hbox{\includegraphics[scale=1]{token.png}}}}
\newcommand{\proposition}{\vcenter{\hbox{\includegraphics[scale=0.8]{proposition.png}}}}

\begin{document}

\begin{preview}

\title{\vspace{-1.5cm} \bfseries\color{blue}{\LARGE Measuring the ``size'' of hypothesis spaces over $\mathbb{R}$ from the perspective of No Free Lunch}}

\author{Yan King Yin} % Your name
% \date{\vspace{-2cm}} % Date, can be changed to a custom date

\maketitle

\setcounter{section}{-1}
\newcounter{mypage}
\setcounter{mypage}{0}

% (1) Circled page number on upper left corner
\begin{textblock*}{5cm}(2.1cm,2.3cm) % {block width} (coords) 
{\color{red}{\large \textcircled{\small \themypage}}}
\addtocounter{mypage}{1}
\end{textblock*}

\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

\section{Motivating example: symmetric functions}

We want to learn, ie, use gradient descent of a neural network to approximate, a symmetric function $y = \hat{f}(\vec{x})$ satisfying the condition $\hat{f}(x_1, ... x_n) = \hat{f}( \sigma \cdot \{x_1, ... x_n\} )$ where $\sigma \in \mathfrak{S}_n$ is a permutation.  The \textbf{input space} is $X = \mathbb{R}^n = n$-dimensional hypercube:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{cube-corner.png}}}
\end{equation}
Permutation symmetry implies that only one \textbf{corner} of the hypercube need be considered, this is the \textbf{fundamental domain} of the symmetry group.  The \textbf{volume} of this domain over the entire hypercube shrinks exponentially as $n$ grows, so it appears that this symmetry is very significant for efficiency consideration.  We want to quantify this notion of efficiency.

As a more specific example, consider only \textbf{Boolean} functions of $n$ variables, so the inputs are the $2^n$ \textbf{vertices} of the hypercube.  It is well-known that the number of $n$-ary Boolean functions is $2^{2^n}$.  It can be seen this way:  the $2^n$ vertices of the hypercube forms the entries of the \textbf{truth table}, ie, all permutations of $\top$ and $\bot$ over input variables.  All we need is to assign output truth-values to these table entries;  each such assignment defines a Boolean function.  Thus $2^n \rightarrow \{ \top, \bot \}$.

The number of \textbf{symmetric} Boolean functions $\hat{f}(x_1, ... x_n)$ does not depend on the order of its arguments.  So it only depends on the \textbf{number} of 1's in the input.  There could be 0, 1, ... up to $n$ 1's in the input, which correspond to vertices of a \textbf{corner} of the $n$-dimension hypercube.  We only need to map this corner to $\{ \top, \bot\}$.  The number of such functions is $2^{n + 1}$.

The ratio of all Boolean functions : symmetric Boolean functions = $2^{2^n} : 2^{n+1} = 2^{2^n - (n+1)}$ of which the linear term is insignificant.  This is a huge number, and this example is illustrative of the general case of functions in $\mathbb{R}^n$ because learning in high dimensions is \textbf{dominated} by the dimensions rather than the ``ups and downs'' along a particular direction.

One last question is that $2n + 1$ seems to be a too-small number of distinct propositions for the purpose of AGI (as each vertex represents a proposition).  To be able to represent more propositions, we can increase the number of points along a single dimension, so the functions are not binary-valued but $k$-ary.  The limit of this process is the continuum, ie $\mathbb{R}^n$ or $[0,1]^n$, the usual setting of neural networks.  As we increase $k$, the number of distinct points in the lower corner is given by ${}_{k+n-1}C_{n}$, ie, the number of ways to choose $n$ objects out of $k$ objects, with replacement, and with order unimportant.  Note that this number grows exponentially as $k$ increases, so it provides many distinct points to represent propositions for AGI.

\section{Finding the right space setting}

We want to compare the \textbf{hypothesis spaces} $A$ and $B$:
\begin{itemize}
	\item $A$ = unconstrained, fully-connected neural network with $L$ layers and $N$ total weights.  The parameter space $A = \Theta_A = \mathbb{R}^N$.
	\item $B$ = symmetric neural network with a special structure but nonetheless its parameter space is also of the form $B = \Theta_B = \mathbb{R}^M$.
\end{itemize}

My first idea is to embed the space $B$ into space $A$.  This will lead to ``unwieldy'' elements of $B$ trying to approximate $A$, for example, consider if $A$'s activation function is sigmoid but $B$'s activation function is RELU.
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{no-free-lunch.png}}}
\end{equation}
But the general idea is: If solutions are only found in space $B$, then limiting our search within $B$ will lead to faster convergence as there is no need to waste time ``wandering'' in space $A$.

A better idea which is much more convenient is to consider $A$ and $B$ as both embedded in an ambient \textbf{function space} $\mathcal{H}$ that is ``fine'' enough to contain all points of $A$ and $B$:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{no-free-lunch-2.png}}} \qquad
\begin{aligned}
& \text{Embeddings:} \\
& {\color{blue}\Theta_A \hookrightarrow} \; \mathcal{H} \\
& {\color{red}\Theta_B \hookrightarrow} \; \mathcal{H} 
\end{aligned}
\end{equation}

\section{Measure roughness of landscape?}

My second idea is to measure the ``roughness'' of the landscapes $A$ and $B$ in the function space $\mathcal{H}$.  The objective function is usually of the form
\begin{equation}
\text{loss} = \sum_i d( f(x_i; \Theta) , y_i ) + \text{Reg}(f)
\end{equation}
where $d$ is some distance metric and Reg is a regularization function.

\begin{wrapfigure}{r}{4cm}
	\centering
	\stepcounter{equation}
	\begin{equation}
	\vcenter{\hbox{\includegraphics[scale=0.75]{gradient-descent-from-hypercube.png}}}
	\tag{\theequation}
	\label{Fig-gradient-descent}
	\end{equation}
	\vspace{-2cm}
\end{wrapfigure}
The models in $A$ or $B$ live in the parameter spaces $\Theta_A$ or $\Theta_B$ and both are embedded into the space $\mathcal{H}$.  However the \textbf{paths} of gradient descent in $A$ or $B$ could still be different, so we have to be cautious about that.

In low-dimension space, we imagine being ``trapped'' in some kind of pit, so we may want to measure the roughness of the landscape by ``undulations'' of gradients.  But our next consideration suggests that this may not be a very effective measurement...

% For some reason I concluded that roughness of landscape is of secondary importance, why?

\section{Gradient descent in high-dimension space}

It may be helpful to visualize gradient descent in high-dimension space with Fig.(\ref{Fig-gradient-descent}).  Imagine the hypercube as having millions of vertices (as the number of weights).  The ``landscape'' is a surface sitting ``over'' the hypercube but we visualize it downwards.

It is very ``easy'' for gradient descent to find a way ``down'' because there are so many dimensions to choose from.

Also, a ``local minimum'' is very rare as it requires millions of gradients to be pointing ``up'' at the same spot.  In a model with a massive number of weights (as in the case of current LLMs), for all practical purposes, a local minimum is just as good as an acceptable solution.  In this sense, \textit{the phenomenon of ``getting stuck in local minima'' disappears}.

\section{The ``symmetric'' parameter space}

The following figure may help visualize the parameter space of our \textbf{symmetric} motivating example (but not for the AGI problem).

In our example, $\Theta_A = (u, v)$ but $\Theta_B$ is just the diagonal space $\langle u = v \rangle$.  The case with 3 parameters, of which 2 are equal, is shown on the right:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{symmetric-space.png}}}
\end{equation}
In general, multiple dimensions can collapse to one, when the weights are shared (equal).

\section{Distribution of solution ``attractors''}

I find the most helpful way to visualize the AGI landscape is to imagine solution \textbf{attractors} to be more \textit{densely} populated in the more-structured space $B$.

The following is a diagram I made up using a Python script and Matplotlib with randomly generated plots.  One can imagine ''attractors'' for \textbf{gradient descent} that are distributed over the hypothesis space that lead to solutions (AGIs).  I made the distribution of attractors in space $B$ \textbf{denser} than in space $A$.  So if an algorithm randomly starts to search, it will have a better chance of success to start within space $B$.  The ``distances'' traveled by gradient descent are also different for $A$ and $B$ because their parameter spaces are different.

\begin{equation}
\vcenter{\hbox{\includegraphics[scale=1]{AGI-attractors.png}}}
\end{equation}
Perhaps the most crucial thing we want to prove, or at least find indirect evidence for, is whether solution attractors are \textbf{sparser} in space $A$ than $B$.  This is obviously a \textbf{problem-specific} question.  In some cases, such as the learning of symmetric functions, we know \textit{a priori} that solutions \textit{exclusively} lie in space $B$.  But the issue is very hard to analyze for AGI.


% 问题 1： 究竟是有很多个解分布在不同区域，还是有一个比较大的单一连通的解区域？  似乎是前者，因为有很多不同的初始 directions 是可以得到 non-identical but effectively equivalent 的解。 

% 问题 2： 是否存在很多 local minima 陷阱？ 其实此问题并不影响我们研究的问题。 而且答案似乎是否定的。

\end{minipage}
\end{preview}

\begin{preview}
\begin{textblock*}{5cm}(2.1cm,2.3cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small \themypage}}}
	\addtocounter{mypage}{1}
\end{textblock*}

\begin{minipage}{\textwidth}
	\setlength{\parskip}{0.4\baselineskip}

\section{Fibration as symmetry}

\textbf{Plan of attack}:  The fibration is a topological structure.  Remember Felix Klein's Erlangen programme in which \textbf{geometric structures} are equated with their respective \textbf{isometry groups of invariance}.  Similarly, we can express a fibration (a topological structure) as an invariance under some appropriate transformation group $G$.  Next, we define a class of functions (supposed to be the neural networks we wish to build) which are invariant under the same group $G$.  By the logic of the Erlangen programme, this is equivalent to saying that these functions are defined over the topological structure of fibrations, which is our goal.

\subsection{Definition of a fiber bundle}

A quadruple $\xi = (X, p, B, F)$ is a \textbf{fiber bundle} if:
\begin{enumerate}[label=(\roman*)]
	\item $X$ is a topological space called the \textbf{total space}
	\item $B$ is a topological space called the \textbf{base space}
	\item $F$ is a topological space called the \textbf{fiber} of $\xi$
	\item $p: X \rightarrow B$ is a continuous surjective map called the \textbf{fiber bundle projection}
	\item the inverse image $p^{-1}(b) = F_b$ is \textbf{homeomorphic} to $ F ,\; \forall b \in B $
	\item $B$ has an open covering $\{ U_i \}_{i \in I}$ such that $\forall i, \; p^{-1}(U_i) $ is \textbf{homeomorphic} to $U_i \times F$, with the projection $p$ yielding the first factor $U_i$.  
\end{enumerate}

We may use the symbol $\cong$ to denote homeomorphism, eg:  $p^{-1}(b) \cong F$.

\subsection{Definition of a bundle morphism}

If $\mathrel{\substack{X\\\downarrow \\B}  {\scriptstyle p}}$ and $\mathrel{\substack{Y\\\downarrow \\A}  {\scriptstyle q}}$ are fiber bundles, then the double map $\sigma = (c,\gamma)$ is a \textbf{bundle morphism} if the following diagram commutes:
\begin{equation}
\begin{tikzcd}[]
	X \arrow[r,"c"] \arrow[d,"p", swap] & Y \arrow[d,"q"] \\
	B \arrow[r,"\gamma"] & A
\end{tikzcd}
\label{bundle-morphism}
\end{equation}

The group $G$ of such morphisms $\sigma \in G$ is the \textbf{transformation group} we're looking for.  We can turn around the definition of a fiber bundle, using the idea of \textbf{invariance} under $G$, as follows:

\subsection{Alternative definition of a fiber bundle (using invariance)}

If $\mathrel{\substack{X\\\downarrow \\B}  {\scriptstyle p}}$ is any projection, and if under any morphism $\sigma = (c,\gamma)$ such that diagram (\ref{bundle-morphism}) commutes, and if the following invariance hold:

\begin{enumerate}[label=(\roman*)]
	\item $\forall b \in B, \exists a \in A. \quad p^{-1}(b) \cong q^{-1}(a) $
	
	\item $\forall i \in I, \exists j \in J. \quad p^{-1}(U_i) \cong q^{-1}(U_j) $
\end{enumerate}
then $\mathrel{\substack{X\\\downarrow \\B}  {\scriptstyle p}}$ is a fiber bundle.

\textbf{\uline{Want}}.  $F$ is defined on a structure which is a bundle.

\textbf{\uline{Have}}.  $ F(x) = F(\sigma \cdot x) \quad \forall \sigma \in G $. $F$ is invariant under a certain transformation group $G$.  Bundle $\xi$ is defined by an invariance under group $G$.  $F$ is defined over what kind of \textbf{space}?  The invariance of $F$ perhaps delineates a fundamental domain and that is the fibration structure?

\end{minipage}
\end{preview}

\begin{preview}
\begin{textblock*}{5cm}(2.1cm,2.3cm) % {block width} (coords) 
{\color{red}{\large \textcircled{\small \themypage}}}
\addtocounter{mypage}{1}
\end{textblock*}

\begin{minipage}{\textwidth}
\setlength{\parskip}{0.4\baselineskip}

\section{Fundamental domains}

The paper \textit{Group-invariant machine learning by fundamental domain projections} [Aslan, Platt, Sheard 2022], by a team of researchers from London, is very inspiring to us.  They study the problem of machine learning over data sets that have group-invariant structure.  One of the simplest such examples is permutation invariance.  There is a way to build neural networks that are \textit{intrinsically} invariant under permutation of input variables.  They are expressed as sums of single-variable functions, which can be shown to be a universal form of all permutation-invariant functions;  its proof involves the Kolmogorov-Arnold representation theorem.

But instead of building such \textit{intrinsic} invariant neural networks, the authors of the paper proposed a novel approach based on \textbf{projections to fundamental domains}.  This can be explained by the following diagram, where $\beta: X \rightarrow Y$ is the 
\begin{equation}
\begin{tikzcd}[column sep=large]
	X \arrow[r,"\beta"] \arrow[d,"\pi", swap] & Y \\
	\overline{\mathcal{F}} \arrow[ur,"\overline{\beta}", swap]
\end{tikzcd}
\end{equation}

\subsection{Some examples of fundamental domains}



The following are examples I created to get an intuitive understanding of fundamental domains for \textbf{continuous groups}:
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.7]{fundamental-domain-examples.png}}}
\end{equation}

\begin{enumerate}[label=(\roman*)]
	\item If the group $G$ = \{ rotations about the origin at various radii \}, then the fundamental domain is any ray from the origin
	
	\item If $G$ = \{ horizontal translations at various heights \}, then the fundamental domain is any vertical straight line
	
	\item Similar to (ii), for vertical translations

	\item If $G$ is the group of (ii) and (iii) combined, then the fundamental domain is just any point, and the function is a constant function.
\end{enumerate}

\end{minipage}
\end{preview}

\begin{preview}
\begin{textblock*}{5cm}(2.1cm,2.3cm) % {block width} (coords) 
	{\color{red}{\large \textcircled{\small \themypage}}}
	\addtocounter{mypage}{1}
\end{textblock*}

\begin{minipage}{\textwidth}
	\setlength{\parskip}{0.4\baselineskip}

\section{This}

\begin{itemize}
	\item Here
\end{itemize}

\end{minipage}
\end{preview}

\end{document}
